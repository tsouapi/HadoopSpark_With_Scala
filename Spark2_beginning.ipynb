{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with default schema (system generator name)\n",
    "val orders=spark.\n",
    "read.\n",
    "csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with option (\"inferSchema\")\n",
    "# the types of each fields are taking into account. \n",
    "val orders=spark.\n",
    "read.\n",
    "option(\"inferSchema\",\"true\").\n",
    "csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with define schema \n",
    "val orders=spark.\n",
    "read.\n",
    "schema(\"\"\" order_id INT, order_date TIMESTAMP,\n",
    "        order_customer_id INT, order_status STRING\n",
    "        \"\"\").\n",
    "csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with define schema and option \n",
    "val orders=spark.\n",
    "read.\n",
    "schema(\"\"\" order_id INT, order_date TIMESTAMP,\n",
    "        order_customer_id INT, order_status STRING\n",
    "        \"\"\").\n",
    "option(\"sep\",\",\").\n",
    "csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with define schema and \n",
    "#option(separator which is comma in this case) and format of the file\n",
    "val orders=spark.\n",
    "read.\n",
    "schema(\"\"\" order_id INT, order_date TIMESTAMP,\n",
    "        order_customer_id INT, order_status STRING\n",
    "        \"\"\").\n",
    "option(\"sep\",\",\").\n",
    "format(\"csv\").\n",
    "load(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'We can built the same with json file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83 - Overview of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using col and lit\n",
    "#import spark.implicit._\n",
    "import org.apache.spark.sql.functions.{col, lit, concat} \n",
    "employeesDF.\n",
    "     select(col(\"employee_id\"), concat(col(\"first_name\"), lit(\" \"),\n",
    "            col(\"last_name\")).alias(\"full_name\"),\n",
    "            col(\"salary\"), col(\"nationality\")\n",
    "            ).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn without dropping \n",
    "employeesDF.\n",
    "      withColumn(\"full_name\",concat(col(\"first_name\"), lit(\" \"),\n",
    "            col(\"last_name\"))\n",
    "            ).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn without drop\n",
    "employeesDF.\n",
    "      withColumn(\"full_name\",concat(col(\"first_name\"), lit(\",\"),\n",
    "            col(\"last_name\"))).\n",
    "            drop(\"first_name\", \"last_name\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with $ with drop\n",
    "employeesDF.\n",
    "      withColumn(\"full_name\",concat($\"first_name\", lit(\",\"),\n",
    "            $\"last_name\")).\n",
    "            drop(\"first_name\", \"last_name\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with SQL to see all functions\n",
    "spark.sql(\"SHOW functions\").show(300, false)\n",
    "# looking the description of the concat function\n",
    "spark.sql(\"DESCRIBE FUNCTION concat\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with SQL \n",
    "employeesDF.\n",
    "      selectExpr(\"employee_id\", \"concat(first_name, ' ', last_name) AS full_name\", \n",
    "                 \"salary\",\"nationality\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the information about the fuile format\n",
    "spark.conf.get(\"spark.sql.parquet.compression.codec\")\n",
    "#res30: String = snappy : mean compress file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using write.parquet\n",
    "orders.\n",
    "    write.\n",
    "    coealesce(1).\n",
    "    option(\"compression\", \"none\").\n",
    "    mode(\"overwrite\").\n",
    "    format(\"parquet\").\n",
    "    save(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 7: Spark2 processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialisation with spark context\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSassion.\n",
    "               builder.\n",
    "            config(\"spark.ui.port\", \"0\").\n",
    "            appName(\" rigobert init spark\").\n",
    "            master(\"yarn\").\n",
    "            getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the case convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.function{concat, col, lit, upper, initcap, lower}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\",\"nationality\").\n",
    "    withColumn(\"nationality_upper\", upper($\"nationality\")).\n",
    "    withColumn(\"nationality_lower\", lower(col(\"nationality\"))).\n",
    "    withColumn(\"nationality_initcap\", initcap(employeesDF(\"nationality\"))).\n",
    "    withColumn(\"nationality_length\", length(col(\"nationality\"))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\",\"phone_number\",\"ssn\").\n",
    "    withColumn(\"phone_last4\",substring($\"phone_number\",-4,4).cast(\"int\")).\n",
    "    withColumn(\"ssn_last8\", substring($\"ssn\",8,4).cast(\"int\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "    select($\"employee_id\",$\"phone_number\",$\"ssn\", substring($\"phone_number\",-4,4).cast(\"int\").alias(\"phone_last4\"),\n",
    "    substring($\"ssn\",8,4).cast(\"int\").alias(\"ssn_last8\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### String manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.function.{split,lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val l=List(\"X\")\n",
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(split(lit(\"hello every one this is rigo tsouapi\"),\" \"){0}).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\",\"phone_number\",\"ssn\").\n",
    "    withColumn(\"area_code\",split($\"phone_number\", \" \"){1}.cast(\"int\")).\n",
    "    withColumn(\"phone_last4\", split($\"phone_number\",\" \"){3}.cast(\"int\")).\n",
    "    withColumn(\"ssn_last4\", split($\"ssn\", \" \"){2}.cast(\"int\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "    select($\"employee_id\",$\"phone_number\",$\"ssn\", split($\"phone_number\",\" \"){0}.cast(\"int\").alias(\"phone_last4\"),\n",
    "    split($\"ssn\",\" \"){1}.cast(\"int\").alias(\"ssn_last8\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatatanate string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.function.{concat,lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicit._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "  withColumn(\"full_name\", concat($\"first_name\",$\"last_name\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "  withColumn(\"fill_name\", concat($\"first_name\", lit(\",\"),$\"last_name\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lpad, rpad\n",
    "import org.apache.park.sql.function.{lit,lpad}\n",
    "val l = List(\"X\")\n",
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(lpad(lit(\"hello world\"),10,\"-\").alias(\"dummy\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(lpad(lit(2), 2,\"0\").alias(\"dummy\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment padding the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val empFixedDF = employeesDF.select(\n",
    "    concat(\n",
    "        lpad($\"employee_id\", 5, \"0\"),\n",
    "        rpad($\"first_name\", 10, \"-\"),\n",
    "        rpad($\"last_name\", 10, \"-\"),\n",
    "        lpad($\"salary\", 10, \"0\"),\n",
    "        rpad($\"nationality\", 15, \"-\"),\n",
    "        rpad($\"phone_number\", 17, \"-\"),\n",
    "        $\"ssn\"\n",
    "    ).alias(\"employee\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trimming function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{concat, ltrim, rtrim, trim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val l= List(\"   Hello.    \")\n",
    "val df=l.toDF(\"dummy\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"ltrim\", ltrim(col(\"dummy\"))).\n",
    "    withColumn(\"rtrim\", rtrim(col(\"dummy\"))).\n",
    "    withColumn(\"trim\", trim(col(\"dummy\"))).\n",
    "    withColumn(\"rmv_dot_trim\", trim(trim(col(\"dummy\")),\".\") # remove dot\n",
    ").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 97 date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, currente_timestamp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val l= List(\"   Hello.    \")\n",
    "val df=l.toDF(\"dummy\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(current_date.alias(\"current_date\")).show\n",
    "df.select(current_timestamp.alias(\"current_timestamp\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and time - Arithmetique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val datetimes = List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{date_add,date_sub, col}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment \n",
    "# Add 10 days to date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"add_date_date\",date_add($\"date\",10)).\n",
    "    withColumn(\"add_date_time\",date_add($\"time\",10)).\n",
    "    withColumn(\"add_sub_date\",date_sub($\"date\", 10)).  #date_add($\"date\",-10)\n",
    "    withColumn(\"add_date_time\",date_add($\"time\",10)\n",
    "              ).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.function.{current_date, current_timestamp, datediff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"datediff_date\", datediff(current_date, $\"date\")).\n",
    "    withColumn(\"datediff_timestamp\", datediff(current_timestamp, $\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.function.{months_between, add_months, round}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"months_between_date\", round(months_between(current_date, $\"date\"),2)).\n",
    "    withColumn(\"months_between_timestamp\",round(months_between(current_timestamp, $\"time\"),2)).\n",
    "    withColumn(\"add_months_date\",add_months($\"date\",3)).\n",
    "    withColumn(\"add_months_time\",add_months($\"time\",3)).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
