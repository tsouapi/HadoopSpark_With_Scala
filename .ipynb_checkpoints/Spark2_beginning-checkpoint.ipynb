{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with default schema (system generator name)\n",
    "val orders=spark.\n",
    "read.\n",
    "csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with option (\"inferSchema\")\n",
    "# the types of each fields are taking into account. \n",
    "val orders=spark.\n",
    "read.\n",
    "option(\"inferSchema\",\"true\").\n",
    "csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with define schema \n",
    "val orders=spark.\n",
    "read.\n",
    "schema(\"\"\" order_id INT, order_date TIMESTAMP,\n",
    "        order_customer_id INT, order_status STRING\n",
    "        \"\"\").\n",
    "csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with define schema and option \n",
    "val orders=spark.\n",
    "read.\n",
    "schema(\"\"\" order_id INT, order_date TIMESTAMP,\n",
    "        order_customer_id INT, order_status STRING\n",
    "        \"\"\").\n",
    "option(\"sep\",\",\").\n",
    "csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of reading script with define schema and \n",
    "#option(separator which is comma in this case) and format of the file\n",
    "val orders=spark.\n",
    "read.\n",
    "schema(\"\"\" order_id INT, order_date TIMESTAMP,\n",
    "        order_customer_id INT, order_status STRING\n",
    "        \"\"\").\n",
    "option(\"sep\",\",\").\n",
    "format(\"csv\").\n",
    "load(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'We can built the same with json file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83 - Overview of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using col and lit\n",
    "#import spark.implicit._\n",
    "import org.apache.spark.sql.functions.{col, lit, concat} \n",
    "employeesDF.\n",
    "     select(col(\"employee_id\"), concat(col(\"first_name\"), lit(\" \"),\n",
    "            col(\"last_name\")).alias(\"full_name\"),\n",
    "            col(\"salary\"), col(\"nationality\")\n",
    "            ).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn without dropping \n",
    "employeesDF.\n",
    "      withColumn(\"full_name\",concat(col(\"first_name\"), lit(\" \"),\n",
    "            col(\"last_name\"))\n",
    "            ).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn without drop\n",
    "employeesDF.\n",
    "      withColumn(\"full_name\",concat(col(\"first_name\"), lit(\",\"),\n",
    "            col(\"last_name\"))).\n",
    "            drop(\"first_name\", \"last_name\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with $ with drop\n",
    "employeesDF.\n",
    "      withColumn(\"full_name\",concat($\"first_name\", lit(\",\"),\n",
    "            $\"last_name\")).\n",
    "            drop(\"first_name\", \"last_name\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with SQL to see all functions\n",
    "spark.sql(\"SHOW functions\").show(300, false)\n",
    "# looking the description of the concat function\n",
    "spark.sql(\"DESCRIBE FUNCTION concat\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with SQL \n",
    "employeesDF.\n",
    "      selectExpr(\"employee_id\", \"concat(first_name, ' ', last_name) AS full_name\", \n",
    "                 \"salary\",\"nationality\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the information about the fuile format\n",
    "spark.conf.get(\"spark.sql.parquet.compression.codec\")\n",
    "#res30: String = snappy : mean compress file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using write.parquet\n",
    "orders.\n",
    "    write.\n",
    "    coealesce(1).\n",
    "    option(\"compression\", \"none\").\n",
    "    mode(\"overwrite\").\n",
    "    format(\"parquet\").\n",
    "    save(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 7: Spark2 processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialisation with spark context\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSassion.\n",
    "               builder.\n",
    "            config(\"spark.ui.port\", \"0\").\n",
    "            appName(\" rigobert init spark\").\n",
    "            master(\"yarn\").\n",
    "            getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the case convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.function{concat, col, lit, upper, initcap, lower}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\",\"nationality\").\n",
    "    withColumn(\"nationality_upper\", upper($\"nationality\")).\n",
    "    withColumn(\"nationality_lower\", lower(col(\"nationality\"))).\n",
    "    withColumn(\"nationality_initcap\", initcap(employeesDF(\"nationality\"))).\n",
    "    withColumn(\"nationality_length\", length(col(\"nationality\"))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
